{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Link to the Youtube tutorial video: https://www.youtube.com/watch?v=8klqIM9UvAc&list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw&index=20\n",
    "\n",
    "1) **Hightlights**:\n",
    "    1) PCA is an unsupervised machine learning algorithm.  <br />  \n",
    "    2) PCA is called dimensionality reduction technique as it can help us reduce dimensions.  <br />\n",
    "    3) PCA is a process of figuring out most important features or principal components that have the most impact on the target (dependent) variable.  <br />  \n",
    "\n",
    "2) **Theory behind PCA**:\n",
    "    1) We take the SKlearn handwritten image dataset as example. The dataset consists of 64 features. Here, we only plot 2 of the features, called corner pixel and central pixel. These clusters represent different digits. You immediately notice that the corner pixel is not playing an important role. The maximum variance (OR maximum variance) is on the y-axis, which is a central pixel.  <br />\n",
    "    <img src=\"hidden\\photo1.png\" alt=\"This image describes PCA, part 1\" style=\"width: 400px;\"/>  <br />\n",
    "\n",
    "    2) If you are asked to reduce this two-dimension into one-dimension, you can easily do so by getting rid of corner pixel, getting the one-dimensional graph shown on right hand side.  <br />\n",
    "        <img src=\"hidden\\photo2.png\" alt=\"This image describes PCA, part 2\" style=\"width: 400px;\"/>  <br />\n",
    "\n",
    "    3) Let's look at the SKlearn iris dataset. If you have a scatterplot as below, you can draw a line (principal component 1, PC1) which covers the maximum variance (OR maximum information in terms of features). Then, you can draw a perpendicular line (principal component 2, PC2) which covers the second most variance. Each principal component is an axis. If there are multiple principal components calculated/drawn/extracted from the dataset, PC1 covers the most variance (most information) in terms of features of the dataset, followed by PC2, PC3, ... <br />\n",
    "        <img src=\"hidden\\photo3.png\" alt=\"This image describes PCA, part 3\" style=\"width: 400px;\"/>  <br />\n",
    "\n",
    "3) **Things to keep in mind before using PCA**:\n",
    "    1) You need to scale the features before applying PCA on them to perform dimensionality reduction. Else, PCA will not work as per your expectation.\n",
    "    2) Accuracy might drop if you use the principal components to train your machine learning model, instead the original features of the dataset.  <br /> \n",
    "        <img src=\"hidden\\photo4.png\" alt=\"This image describes PCA, part 4\" style=\"width: 400px;\"/>  <br />\n",
    "\n",
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the attributes of the dataset\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 1797 samples/rows, while each sample consists of 64 features/colums.\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the data attribute of the dataset. The data attribute contains the independent variables (features) of the dataset.\n",
    "print('The dataset consists of '+str(dataset.data.shape[0])+' samples/rows, while each sample consists of '+str(dataset.data.shape[1])+' features/colums.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the 1st sample of the dataset:\n",
      " [ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "The features of the 1st sample of the dataset:\n",
      " [[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# show the features of the 1st sample of the dataset, in 1D array format\n",
    "print('The features of the 1st sample of the dataset:\\n', dataset.data[0])\n",
    "\n",
    "# show the features of the 1st sample of the dataset, in 2D array format\n",
    "print('The features of the 1st sample of the dataset:\\n', dataset.data[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27e65b81ca0>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAGkCAYAAAAIduO+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYoElEQVR4nO3df2yUhR3H8c/R2oNpexak0I7jpygCtoMWCKvOHyCkQSL7oxKCWYXNRXJMsDFx/WewLOPqH1twGyk/xoqJYyDLis4MusKkZJkdpaQJaIJgmRwidG5wV7rkML3bX7utQ9o+R788PNf3K3midz7HfUIqb+5He75kMpkUAABGhrk9AACQ2QgNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAVMaEZsuWLZo4caKGDx+uefPm6dixY25P6tfRo0e1dOlSFRUVyefzaf/+/W5PGpBwOKw5c+YoNzdXBQUFWrZsmU6fPu32rAGpq6tTcXGx8vLylJeXp/nz5+vAgQNuz3KstrZWPp9P69evd3tKvzZu3Cifz9frmDZtmtuzBuTTTz/Vc889p1GjRmnEiBF6+OGHdfz4cbdn9WvixIk3/J77fD6FQiFX9mREaPbu3avq6mpt2LBBJ06cUElJiRYvXqzOzk63p/Wpu7tbJSUl2rJli9tTHGlublYoFFJLS4uampr0xRdfaNGiReru7nZ7Wr/GjRun2tpatbW16fjx43ryySf1zDPP6IMPPnB72oC1trZq27ZtKi4udnvKgM2YMUOfffZZ6vjzn//s9qR+XblyReXl5brrrrt04MABffjhh/rJT36i/Px8t6f1q7W1tdfvd1NTkySpsrLSnUHJDDB37txkKBRKXe7p6UkWFRUlw+Gwi6uckZRsaGhwe0ZaOjs7k5KSzc3Nbk9JS35+fvKXv/yl2zMGpKurKzl16tRkU1NT8rHHHkuuW7fO7Un92rBhQ7KkpMTtGY69+uqryUceecTtGYNi3bp1ySlTpiQTiYQr9+/5RzTXr19XW1ubFi5cmLpu2LBhWrhwod5//30Xlw0d0WhUkjRy5EiXlzjT09OjPXv2qLu7W/Pnz3d7zoCEQiEtWbKk19e7F5w5c0ZFRUWaPHmyVq5cqfPnz7s9qV/vvPOOysrKVFlZqYKCAs2aNUs7duxwe5Zj169f15tvvqnVq1fL5/O5ssHzofn888/V09OjMWPG9Lp+zJgxunTpkkurho5EIqH169ervLxcM2fOdHvOgJw8eVL33HOP/H6/XnzxRTU0NGj69Oluz+rXnj17dOLECYXDYbenODJv3jzt2rVLBw8eVF1dnc6dO6dHH31UXV1dbk/rU0dHh+rq6jR16lQ1NjZqzZo1eumll/TGG2+4Pc2R/fv36+rVq3r++edd25Dt2j0jI4RCIZ06dcoTz7n/x4MPPqj29nZFo1H99re/VVVVlZqbm+/o2EQiEa1bt05NTU0aPny423McqaioSP17cXGx5s2bpwkTJuitt97St7/9bReX9S2RSKisrEybNm2SJM2aNUunTp3S1q1bVVVV5fK6gdu5c6cqKipUVFTk2gbPP6K57777lJWVpcuXL/e6/vLlyxo7dqxLq4aGtWvX6t1339V7772ncePGuT1nwHJycnT//fertLRU4XBYJSUlev31192e1ae2tjZ1dnZq9uzZys7OVnZ2tpqbm/Wzn/1M2dnZ6unpcXvigN1777164IEHdPbsWben9KmwsPCGv3w89NBDnnja7z8++eQTHTp0SN/5zndc3eH50OTk5Ki0tFSHDx9OXZdIJHT48GHPPO/uNclkUmvXrlVDQ4P+9Kc/adKkSW5PuiWJRELxeNztGX1asGCBTp48qfb29tRRVlamlStXqr29XVlZWW5PHLBr167p448/VmFhodtT+lReXn7D2/Y/+ugjTZgwwaVFztXX16ugoEBLlixxdUdGPHVWXV2tqqoqlZWVae7cudq8ebO6u7u1atUqt6f16dq1a73+Vnfu3Dm1t7dr5MiRGj9+vIvL+hYKhbR79269/fbbys3NTb0WFggENGLECJfX9a2mpkYVFRUaP368urq6tHv3bh05ckSNjY1uT+tTbm7uDa+B3X333Ro1atQd/9rYK6+8oqVLl2rChAm6ePGiNmzYoKysLK1YscLtaX16+eWX9fWvf12bNm3Ss88+q2PHjmn79u3avn2729MGJJFIqL6+XlVVVcrOdvmPelfe62bg5z//eXL8+PHJnJyc5Ny5c5MtLS1uT+rXe++9l5R0w1FVVeX2tD592WZJyfr6eren9Wv16tXJCRMmJHNycpKjR49OLliwIPnHP/7R7Vlp8crbm5cvX54sLCxM5uTkJL/61a8mly9fnjx79qzbswbk97//fXLmzJlJv9+fnDZtWnL79u1uTxqwxsbGpKTk6dOn3Z6S9CWTyaQ7iQMADAWef40GAHBnIzQAAFOEBgBgitAAAEwRGgCAKUIDADCVUaGJx+PauHHjHf9d3v/Pq7sl72736m7Ju9u9ulvy7vY7ZXdGfR9NLBZTIBBQNBpVXl6e23MGzKu7Je9u9+puybvbvbpb8u72O2V3Rj2iAQDceQgNAMDUbf9Ja4lEQhcvXlRubu6gf9pbLBbr9U+v8Opuybvbvbpb8u52r+6WvLvdencymVRXV5eKioo0bNjNH7fc9tdoLly4oGAweDvvEgBgKBKJ9PmZVLf9EU1ubu7tvktIWrZsmdsT0rJx40a3J6TtyJEjbk9Ii5d/z69ever2hCGpvz/Xb3toBvvpMgzMXXfd5faEtHj5LyZ3+mfz3Az/j8Kp/r5meDMAAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACm0grNli1bNHHiRA0fPlzz5s3TsWPHBnsXACBDOA7N3r17VV1drQ0bNujEiRMqKSnR4sWL1dnZabEPAOBxjkPz05/+VC+88IJWrVql6dOna+vWrfrKV76iX/3qVxb7AAAe5yg0169fV1tbmxYuXPjfX2DYMC1cuFDvv//+l94mHo8rFov1OgAAQ4ej0Hz++efq6enRmDFjel0/ZswYXbp06UtvEw6HFQgEUkcwGEx/LQDAc8zfdVZTU6NoNJo6IpGI9V0CAO4g2U5Ovu+++5SVlaXLly/3uv7y5csaO3bsl97G7/fL7/envxAA4GmOHtHk5OSotLRUhw8fTl2XSCR0+PBhzZ8/f9DHAQC8z9EjGkmqrq5WVVWVysrKNHfuXG3evFnd3d1atWqVxT4AgMc5Ds3y5cv197//XT/4wQ906dIlfe1rX9PBgwdveIMAAABSGqGRpLVr12rt2rWDvQUAkIH4WWcAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJhK64PP4D21tbVuT0jL5MmT3Z6Qtvz8fLcnpOWf//yn2xPS9uyzz7o9IS379u1ze4IpHtEAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMOU4NEePHtXSpUtVVFQkn8+n/fv3G8wCAGQKx6Hp7u5WSUmJtmzZYrEHAJBhsp3eoKKiQhUVFRZbAAAZyHFonIrH44rH46nLsVjM+i4BAHcQ8zcDhMNhBQKB1BEMBq3vEgBwBzEPTU1NjaLRaOqIRCLWdwkAuIOYP3Xm9/vl9/ut7wYAcIfi+2gAAKYcP6K5du2azp49m7p87tw5tbe3a+TIkRo/fvygjgMAeJ/j0Bw/flxPPPFE6nJ1dbUkqaqqSrt27Rq0YQCAzOA4NI8//riSyaTFFgBABuI1GgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATDn+4LOhrLS01O0JaZs8ebLbE9IyZcoUtyekraOjw+0JaWlqanJ7Qtq8+v/ovn373J5gikc0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgylFowuGw5syZo9zcXBUUFGjZsmU6ffq01TYAQAZwFJrm5maFQiG1tLSoqalJX3zxhRYtWqTu7m6rfQAAj8t2cvLBgwd7Xd61a5cKCgrU1tamb3zjG4M6DACQGRyF5v9Fo1FJ0siRI296TjweVzweT12OxWK3cpcAAI9J+80AiURC69evV3l5uWbOnHnT88LhsAKBQOoIBoPp3iUAwIPSDk0oFNKpU6e0Z8+ePs+rqalRNBpNHZFIJN27BAB4UFpPna1du1bvvvuujh49qnHjxvV5rt/vl9/vT2scAMD7HIUmmUzqe9/7nhoaGnTkyBFNmjTJahcAIEM4Ck0oFNLu3bv19ttvKzc3V5cuXZIkBQIBjRgxwmQgAMDbHL1GU1dXp2g0qscff1yFhYWpY+/evVb7AAAe5/ipMwAAnOBnnQEATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYMrRB58Ndfn5+W5PSFtbW5vbE9LS0dHh9oQhx6tfK7hz8YgGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClHoamrq1NxcbHy8vKUl5en+fPn68CBA1bbAAAZwFFoxo0bp9raWrW1ten48eN68skn9cwzz+iDDz6w2gcA8LhsJycvXbq01+Uf//jHqqurU0tLi2bMmDGowwAAmcFRaP5XT0+P9u3bp+7ubs2fP/+m58XjccXj8dTlWCyW7l0CADzI8ZsBTp48qXvuuUd+v18vvviiGhoaNH369JueHw6HFQgEUkcwGLylwQAAb3EcmgcffFDt7e3661//qjVr1qiqqkoffvjhTc+vqalRNBpNHZFI5JYGAwC8xfFTZzk5Obr//vslSaWlpWptbdXrr7+ubdu2fen5fr9ffr//1lYCADzrlr+PJpFI9HoNBgCA/+XoEU1NTY0qKio0fvx4dXV1affu3Tpy5IgaGxut9gEAPM5RaDo7O/Wtb31Ln332mQKBgIqLi9XY2KinnnrKah8AwOMchWbnzp1WOwAAGYqfdQYAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClHH3w21OXn57s9IW2HDh1yewI8wstf51euXHF7Ar4Ej2gAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMDULYWmtrZWPp9P69evH6Q5AIBMk3ZoWltbtW3bNhUXFw/mHgBAhkkrNNeuXdPKlSu1Y8cO5efnD/YmAEAGSSs0oVBIS5Ys0cKFC/s9Nx6PKxaL9ToAAENHttMb7NmzRydOnFBra+uAzg+Hw/rhD3/oeBgAIDM4ekQTiUS0bt06/frXv9bw4cMHdJuamhpFo9HUEYlE0hoKAPAmR49o2tra1NnZqdmzZ6eu6+np0dGjR/WLX/xC8XhcWVlZvW7j9/vl9/sHZy0AwHMchWbBggU6efJkr+tWrVqladOm6dVXX70hMgAAOApNbm6uZs6c2eu6u+++W6NGjbrhegAAJH4yAADAmON3nf2/I0eODMIMAECm4hENAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmbvmDz4aSK1euuD0hbaWlpW5PGHLy8/PdnpAWL3+t7Nu3z+0J+BI8ogEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgylFoNm7cKJ/P1+uYNm2a1TYAQAbIdnqDGTNm6NChQ//9BbId/xIAgCHEcSWys7M1duxYiy0AgAzk+DWaM2fOqKioSJMnT9bKlSt1/vz5Ps+Px+OKxWK9DgDA0OEoNPPmzdOuXbt08OBB1dXV6dy5c3r00UfV1dV109uEw2EFAoHUEQwGb3k0AMA7HIWmoqJClZWVKi4u1uLFi/WHP/xBV69e1VtvvXXT29TU1CgajaaOSCRyy6MBAN5xS6/k33vvvXrggQd09uzZm57j9/vl9/tv5W4AAB52S99Hc+3aNX388ccqLCwcrD0AgAzjKDSvvPKKmpub9be//U1/+ctf9M1vflNZWVlasWKF1T4AgMc5eurswoULWrFihf7xj39o9OjReuSRR9TS0qLRo0db7QMAeJyj0OzZs8dqBwAgQ/GzzgAApggNAMAUoQEAmCI0AABThAYAYIrQAABMERoAgClCAwAwRWgAAKYIDQDAFKEBAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMOXog8+Guo6ODrcnpK20tNTtCWmprKx0e0LavLzdq1577TW3J+BL8IgGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMOQ7Np59+queee06jRo3SiBEj9PDDD+v48eMW2wAAGSDbyclXrlxReXm5nnjiCR04cECjR4/WmTNnlJ+fb7UPAOBxjkLz2muvKRgMqr6+PnXdpEmTBn0UACBzOHrq7J133lFZWZkqKytVUFCgWbNmaceOHX3eJh6PKxaL9ToAAEOHo9B0dHSorq5OU6dOVWNjo9asWaOXXnpJb7zxxk1vEw6HFQgEUkcwGLzl0QAA73AUmkQiodmzZ2vTpk2aNWuWvvvd7+qFF17Q1q1bb3qbmpoaRaPR1BGJRG55NADAOxyFprCwUNOnT+913UMPPaTz58/f9DZ+v195eXm9DgDA0OEoNOXl5Tp9+nSv6z766CNNmDBhUEcBADKHo9C8/PLLamlp0aZNm3T27Fnt3r1b27dvVygUstoHAPA4R6GZM2eOGhoa9Jvf/EYzZ87Uj370I23evFkrV6602gcA8DhH30cjSU8//bSefvppiy0AgAzEzzoDAJgiNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMCU4w8+G8o6OjrcnpC273//+25PSEttba3bE9LW1tbm9oS0lJWVuT0BGYZHNAAAU4QGAGCK0AAATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABThAYAYIrQAABMOQrNxIkT5fP5bjhCoZDVPgCAx2U7Obm1tVU9PT2py6dOndJTTz2lysrKQR8GAMgMjkIzevToXpdra2s1ZcoUPfbYY4M6CgCQORyF5n9dv35db775pqqrq+Xz+W56XjweVzweT12OxWLp3iUAwIPSfjPA/v37dfXqVT3//PN9nhcOhxUIBFJHMBhM9y4BAB6Udmh27typiooKFRUV9XleTU2NotFo6ohEIuneJQDAg9J66uyTTz7RoUOH9Lvf/a7fc/1+v/x+fzp3AwDIAGk9oqmvr1dBQYGWLFky2HsAABnGcWgSiYTq6+tVVVWl7Oy030sAABgiHIfm0KFDOn/+vFavXm2xBwCQYRw/JFm0aJGSyaTFFgBABuJnnQEATBEaAIApQgMAMEVoAACmCA0AwBShAQCYIjQAAFOEBgBgitAAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABTt/0jMvksG3dcv37d7Qlp6erqcntC2v71r3+5PQG4Lfr7c92XvM1/8l+4cEHBYPB23iUAwFAkEtG4ceNu+t9ve2gSiYQuXryo3Nxc+Xy+Qf21Y7GYgsGgIpGI8vLyBvXXtuTV3ZJ3t3t1t+Td7V7dLXl3u/XuZDKprq4uFRUVadiwm78Sc9ufOhs2bFif5RsMeXl5nvpi+A+v7pa8u92ruyXvbvfqbsm72y13BwKBfs/hzQAAAFOEBgBgKqNC4/f7tWHDBvn9frenOOLV3ZJ3t3t1t+Td7V7dLXl3+52y+7a/GQAAMLRk1CMaAMCdh9AAAEwRGgCAKUIDADBFaAAApggNAMAUoQEAmCI0AABT/wYMQUBqKDC9pAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the actual image of the 1st sample of the dataset\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the colorspace of the figure window to gray\n",
    "plt.gray()\n",
    "\n",
    "# plot the actual image of the sample \n",
    "plt.matshow(dataset.data[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "The possible label names of the dataset:\n",
      " [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# show the data (labels/classes) available in the target attribute of the dataset. The target attribute contains the dependent variable (target) of the dataset.\n",
    "print(dataset.target)\n",
    "\n",
    "# show all the possible label/class names in the target attribute of the dataset \n",
    "import numpy as np\n",
    "print('The possible label names of the dataset:\\n',np.unique(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "\n",
       "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
       "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0  ...        5.0        0.0   \n",
       "3        0.0        0.0        0.0        8.0  ...        9.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "   pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
       "0        0.0        0.0        6.0       13.0       10.0        0.0   \n",
       "1        0.0        0.0        0.0       11.0       16.0       10.0   \n",
       "2        0.0        0.0        0.0        3.0       11.0       16.0   \n",
       "3        0.0        0.0        7.0       13.0       13.0        9.0   \n",
       "4        0.0        0.0        0.0        2.0       16.0        4.0   \n",
       "\n",
       "   pixel_7_6  pixel_7_7  \n",
       "0        0.0        0.0  \n",
       "1        0.0        0.0  \n",
       "2        9.0        0.0  \n",
       "3        0.0        0.0  \n",
       "4        0.0        0.0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For the column name of pixel_X_Y, X represents the number of rows while \n",
    "Y represents the number of columns where the pixel is located. \n",
    "Since the dataset consists of 8x8 images, \n",
    "the column names of the dataset range from (X=0,Y=0) to (X=7,Y=7).\n",
    "'''\n",
    "\n",
    "# create a dataframe called df, which consists of the features of the dataset\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "\n",
    "# show the first 5 rows of the df dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1797.0</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303840</td>\n",
       "      <td>5.204786</td>\n",
       "      <td>11.835838</td>\n",
       "      <td>11.848080</td>\n",
       "      <td>5.781859</td>\n",
       "      <td>1.362270</td>\n",
       "      <td>0.129661</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>1.993879</td>\n",
       "      <td>...</td>\n",
       "      <td>3.725097</td>\n",
       "      <td>0.206455</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.279354</td>\n",
       "      <td>5.557596</td>\n",
       "      <td>12.089037</td>\n",
       "      <td>11.809126</td>\n",
       "      <td>6.764051</td>\n",
       "      <td>2.067891</td>\n",
       "      <td>0.364496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907192</td>\n",
       "      <td>4.754826</td>\n",
       "      <td>4.248842</td>\n",
       "      <td>4.287388</td>\n",
       "      <td>5.666418</td>\n",
       "      <td>3.325775</td>\n",
       "      <td>1.037383</td>\n",
       "      <td>0.094222</td>\n",
       "      <td>3.196160</td>\n",
       "      <td>...</td>\n",
       "      <td>4.919406</td>\n",
       "      <td>0.984401</td>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.934302</td>\n",
       "      <td>5.103019</td>\n",
       "      <td>4.374694</td>\n",
       "      <td>4.933947</td>\n",
       "      <td>5.900623</td>\n",
       "      <td>4.090548</td>\n",
       "      <td>1.860122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel_0_0    pixel_0_1    pixel_0_2    pixel_0_3    pixel_0_4  \\\n",
       "count     1797.0  1797.000000  1797.000000  1797.000000  1797.000000   \n",
       "mean         0.0     0.303840     5.204786    11.835838    11.848080   \n",
       "std          0.0     0.907192     4.754826     4.248842     4.287388   \n",
       "min          0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "25%          0.0     0.000000     1.000000    10.000000    10.000000   \n",
       "50%          0.0     0.000000     4.000000    13.000000    13.000000   \n",
       "75%          0.0     0.000000     9.000000    15.000000    15.000000   \n",
       "max          0.0     8.000000    16.000000    16.000000    16.000000   \n",
       "\n",
       "         pixel_0_5    pixel_0_6    pixel_0_7    pixel_1_0    pixel_1_1  ...  \\\n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  ...   \n",
       "mean      5.781859     1.362270     0.129661     0.005565     1.993879  ...   \n",
       "std       5.666418     3.325775     1.037383     0.094222     3.196160  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       4.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%      11.000000     0.000000     0.000000     0.000000     3.000000  ...   \n",
       "max      16.000000    16.000000    15.000000     2.000000    16.000000  ...   \n",
       "\n",
       "         pixel_6_6    pixel_6_7    pixel_7_0    pixel_7_1    pixel_7_2  \\\n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000   \n",
       "mean      3.725097     0.206455     0.000556     0.279354     5.557596   \n",
       "std       4.919406     0.984401     0.023590     0.934302     5.103019   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     4.000000   \n",
       "75%       7.000000     0.000000     0.000000     0.000000    10.000000   \n",
       "max      16.000000    13.000000     1.000000     9.000000    16.000000   \n",
       "\n",
       "         pixel_7_3    pixel_7_4    pixel_7_5    pixel_7_6    pixel_7_7  \n",
       "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  \n",
       "mean     12.089037    11.809126     6.764051     2.067891     0.364496  \n",
       "std       4.374694     4.933947     5.900623     4.090548     1.860122  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%      11.000000    10.000000     0.000000     0.000000     0.000000  \n",
       "50%      13.000000    14.000000     6.000000     0.000000     0.000000  \n",
       "75%      16.000000    16.000000    12.000000     2.000000     0.000000  \n",
       "max      16.000000    16.000000    16.000000    16.000000    16.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The last row of the output shows the maximum value of each entry of each column is 16. This is because in gray colorspace, pixel value of 16 represents the white color, pixel value of 0 represents the black color.\n",
    "'''\n",
    "\n",
    "# show the statistics of values of each column of the df dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the independent variables (features) of the dataset to X variable\n",
    "X = df\n",
    "\n",
    "# store the dependent variable of the dataset to Y variable\n",
    "Y = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale each feature of the dataset using StandardScaler\n",
    "\n",
    "1) Features scaling is a common thing to do before training a machine learning model\n",
    "2) If the scale of features of the dataset used to train a machine learning model is not correct (EG: If a machine learning model is trained with only 2 features, scale of the feature represented by x-axis is different with the scale of the feature represented by y-axis), the machine learning model cannot predict the new samples/data points correctly. \n",
    "3) Hence, we must scale the features of the dataset (which used to train the machine learning model) [Here, are the 64 columns available in the data attribute of the dataset] before training the model and use the model to make predictions, so that the model can predict the new samples/data points correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-0.043081</td>\n",
       "      <td>0.274072</td>\n",
       "      <td>-0.664478</td>\n",
       "      <td>-0.844129</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.757436</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>0.086719</td>\n",
       "      <td>0.208293</td>\n",
       "      <td>-0.366771</td>\n",
       "      <td>-1.146647</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-1.094937</td>\n",
       "      <td>0.038648</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.757436</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-1.089383</td>\n",
       "      <td>-0.249010</td>\n",
       "      <td>0.849632</td>\n",
       "      <td>0.548561</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-1.094937</td>\n",
       "      <td>-1.844742</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>1.097673</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259230</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-1.089383</td>\n",
       "      <td>-2.078218</td>\n",
       "      <td>-0.164037</td>\n",
       "      <td>1.565686</td>\n",
       "      <td>1.695137</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>0.377661</td>\n",
       "      <td>0.744919</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>-0.844129</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>1.879691</td>\n",
       "      <td>...</td>\n",
       "      <td>1.072563</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>0.282736</td>\n",
       "      <td>0.208293</td>\n",
       "      <td>0.241430</td>\n",
       "      <td>0.379040</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-1.094937</td>\n",
       "      <td>-2.551014</td>\n",
       "      <td>-0.197863</td>\n",
       "      <td>-1.020657</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.757436</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-1.089383</td>\n",
       "      <td>-2.306869</td>\n",
       "      <td>0.849632</td>\n",
       "      <td>-0.468564</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-0.253452</td>\n",
       "      <td>-0.432200</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>0.038508</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.311047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055897</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-0.697349</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.646898</td>\n",
       "      <td>0.379040</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>0.167290</td>\n",
       "      <td>0.980343</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>0.921145</td>\n",
       "      <td>-0.108958</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554103</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>0.086719</td>\n",
       "      <td>0.894246</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>-0.129523</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-0.884566</td>\n",
       "      <td>-0.196776</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>-0.844129</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.757436</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-0.697349</td>\n",
       "      <td>-0.706312</td>\n",
       "      <td>0.241430</td>\n",
       "      <td>-0.129523</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>-0.674195</td>\n",
       "      <td>-0.432200</td>\n",
       "      <td>-1.131092</td>\n",
       "      <td>-1.020657</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>-0.624009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350769</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>-0.299081</td>\n",
       "      <td>-0.109298</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.849632</td>\n",
       "      <td>0.887602</td>\n",
       "      <td>-0.505670</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.335016</td>\n",
       "      <td>1.008775</td>\n",
       "      <td>0.509495</td>\n",
       "      <td>-0.897785</td>\n",
       "      <td>-0.844129</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.125023</td>\n",
       "      <td>-0.059078</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869229</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>-0.023596</td>\n",
       "      <td>0.771535</td>\n",
       "      <td>0.478753</td>\n",
       "      <td>-0.020358</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.887602</td>\n",
       "      <td>-0.261136</td>\n",
       "      <td>-0.196008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6   \\\n",
       "0     0.0 -0.335016 -0.043081  0.274072 -0.664478 -0.844129 -0.409724   \n",
       "1     0.0 -0.335016 -1.094937  0.038648  0.268751 -0.138020 -0.409724   \n",
       "2     0.0 -0.335016 -1.094937 -1.844742  0.735366  1.097673 -0.409724   \n",
       "3     0.0 -0.335016  0.377661  0.744919  0.268751 -0.844129 -0.409724   \n",
       "4     0.0 -0.335016 -1.094937 -2.551014 -0.197863 -1.020657 -0.409724   \n",
       "...   ...       ...       ...       ...       ...       ...       ...   \n",
       "1792  0.0 -0.335016 -0.253452 -0.432200  0.268751  0.038508 -0.409724   \n",
       "1793  0.0 -0.335016  0.167290  0.980343  0.268751  0.921145 -0.108958   \n",
       "1794  0.0 -0.335016 -0.884566 -0.196776  0.735366 -0.844129 -0.409724   \n",
       "1795  0.0 -0.335016 -0.674195 -0.432200 -1.131092 -1.020657 -0.409724   \n",
       "1796  0.0 -0.335016  1.008775  0.509495 -0.897785 -0.844129 -0.409724   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0    -0.125023 -0.059078 -0.624009  ... -0.757436 -0.209785 -0.023596   \n",
       "1    -0.125023 -0.059078 -0.624009  ... -0.757436 -0.209785 -0.023596   \n",
       "2    -0.125023 -0.059078 -0.624009  ...  0.259230 -0.209785 -0.023596   \n",
       "3    -0.125023 -0.059078  1.879691  ...  1.072563 -0.209785 -0.023596   \n",
       "4    -0.125023 -0.059078 -0.624009  ... -0.757436 -0.209785 -0.023596   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1792 -0.125023 -0.059078 -0.311047  ...  0.055897 -0.209785 -0.023596   \n",
       "1793 -0.125023 -0.059078 -0.624009  ... -0.554103 -0.209785 -0.023596   \n",
       "1794 -0.125023 -0.059078 -0.624009  ... -0.757436 -0.209785 -0.023596   \n",
       "1795 -0.125023 -0.059078 -0.624009  ... -0.350769 -0.209785 -0.023596   \n",
       "1796 -0.125023 -0.059078  0.001916  ...  0.869229 -0.209785 -0.023596   \n",
       "\n",
       "            57        58        59        60        61        62        63  \n",
       "0    -0.299081  0.086719  0.208293 -0.366771 -1.146647 -0.505670 -0.196008  \n",
       "1    -0.299081 -1.089383 -0.249010  0.849632  0.548561 -0.505670 -0.196008  \n",
       "2    -0.299081 -1.089383 -2.078218 -0.164037  1.565686  1.695137 -0.196008  \n",
       "3    -0.299081  0.282736  0.208293  0.241430  0.379040 -0.505670 -0.196008  \n",
       "4    -0.299081 -1.089383 -2.306869  0.849632 -0.468564 -0.505670 -0.196008  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1792 -0.299081 -0.697349  0.436944  0.646898  0.379040 -0.505670 -0.196008  \n",
       "1793 -0.299081  0.086719  0.894246  0.444164 -0.129523 -0.505670 -0.196008  \n",
       "1794 -0.299081 -0.697349 -0.706312  0.241430 -0.129523 -0.505670 -0.196008  \n",
       "1795 -0.299081 -0.109298 -0.020358  0.849632  0.887602 -0.505670 -0.196008  \n",
       "1796  0.771535  0.478753 -0.020358  0.444164  0.887602 -0.261136 -0.196008  \n",
       "\n",
       "[1797 rows x 64 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler with the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "'''\n",
    "**make the scale of the features available in the data attribute of the dataset into the range from 0 to 1 (so after the scaling for the features is done, you will have a scale of 0 to 1 for each of the features)**\n",
    "With fit_transform(), the StandardScaler first learns how to scale the features of the dataset (by calculating the mean and standard deviation for each feature).\n",
    "Then scales (transforms) the data (each entry of the feature columns, available in X dataframe) into a more suitable form (in the range from 0 to 1), according to the mean and standard deviation calculated during the fit() step. \n",
    "The scaled features are stored in the X_scaled variable. \n",
    "'''\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# show the scaled features in the format of dataframe\n",
    "pd.DataFrame(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop the machiner learning model (Logistic regression model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model:  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into train and test sets using train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=30)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create the logistic regression model\n",
    "model_withoutPCA = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "model_withoutPCA.fit(X_train,Y_train)\n",
    "\n",
    "# show the accuracy of the trained model\n",
    "print('The accuracy of the trained model: ',model_withoutPCA.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With dimensionality reduction using PCA\n",
    "\n",
    "The independent variables (features) provided to the PCA model for dimensionality reduction must not be scaled (means must be original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With percentage of variance as argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X dataframe (the dataframe consists of all features of the dataset) consists of 64 features/columns for the 1797 samples of the dataset.\n",
      "After dimensionality reduction using PCA, the X_pca dataframe consists of 40 principal components/features created by the PCA model/columns for the 1797 samples of the dataset.\n",
      "This means the PCA model reduced the features of the dataset from 64 (original features of the dataset) to 40 (newly computed features by the PCA model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''\n",
    "create the PCA model, with the percentage of variance (variation) as argument. \n",
    "The 0.95 argument tells the PCA model to retain 95% of features (variation) of the dataset, and then use them to create new dimensions (principal components).\n",
    "'''\n",
    "pca_percent = PCA(0.95)\n",
    "\n",
    "'''\n",
    "With fit_transform(), the PCA model first learns the features of the dataset.\n",
    "Then, it reduces (transforms) the available features of the dataset into the percentage you specified. In other words, it retains the percentage of features you specified.\n",
    "The PCA model then transforms the retained features into principal components (features created by PCA model) in the form of dataframe, before saving it to X_pca variable.\n",
    "'''\n",
    "X_pca_percent = pca_percent.fit_transform(X_scaled)\n",
    "\n",
    "print('The X dataframe (the dataframe consists of all features of the dataset) consists of '+ str(X.shape[1]) +' features/columns for the ' +str(X.shape[0])+' samples of the dataset.')\n",
    "print('After dimensionality reduction using PCA, the X_pca dataframe consists of '+ str(X_pca_percent.shape[1]) +' principal components/features created by the PCA model/columns for the ' +str(X_pca_percent.shape[0])+' samples of the dataset.')\n",
    "print('This means the PCA model reduced the features of the dataset from '+ str(X.shape[1]) +' (original features of the dataset) to '+ str(X_pca_percent.shape[1]) + ' (newly computed features by the PCA model).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the outputs from the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of principal components (features created by the PCA model) available:  40\n",
      "The percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model:\n",
      " [0.12033916 0.09561054 0.08444415 0.06498408 0.04860155 0.0421412\n",
      " 0.03942083 0.03389381 0.02998221 0.02932003 0.02781805 0.02577055\n",
      " 0.02275303 0.0222718  0.02165229 0.01914167 0.01775547 0.01638069\n",
      " 0.0159646  0.01489191 0.0134797  0.01271931 0.01165837 0.01057647\n",
      " 0.00975316 0.00944559 0.00863014 0.00836643 0.00797693 0.00746471\n",
      " 0.00725582 0.00691911 0.00653909 0.00640793 0.00591384 0.00571162\n",
      " 0.00523637 0.00481808 0.00453719 0.00423163]\n"
     ]
    }
   ],
   "source": [
    "print('The number of principal components (features created by the PCA model) available: ', pca_percent.n_components_)\n",
    "\n",
    "'''\n",
    "**show the percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model**\n",
    "The first value represents the percentage of variance for the 1st principal component. \n",
    "The output shows the PCA model captured the data of 1st principal component consists of 14.9% of variance.\n",
    "'''\n",
    "print('The percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model:\\n', pca_percent.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model:  0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and test sets using train_test_split\n",
    "X_train_pca_percent, X_test_pca_percent, Y_train_pca_percent, Y_test_pca_percent = train_test_split(X_pca_percent, Y, test_size=0.2, random_state=30)\n",
    "\n",
    "# create the logistic regression model. max_iter=1000 is set here to prevent lbfgs failed to converge warning (in other words, this warning means the gradient descent cannot converge to find the minima. the optimal parameters)\n",
    "model_pca_percent = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# train the model\n",
    "model_pca_percent.fit(X_train_pca_percent,Y_train_pca_percent)\n",
    "\n",
    "# show the accuracy of the trained model\n",
    "print('The accuracy of the trained model: ',model_pca_percent.score(X_test_pca_percent,Y_test_pca_percent))\n",
    "\n",
    "# the accuracy here shows that despite dropping lots of features, the accuracy of the model trained with reduced features is similar to the one trained witl original features without dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With number of principal components as argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X dataframe (the dataframe consists of all features of the dataset) consists of 64 features/columns for the 1797 samples of the dataset.\n",
      "After dimensionality reduction using PCA, the X_pca dataframe consists of 2 principal components/features created by the PCA model/columns for the 1797 samples of the dataset.\n",
      "This means the PCA model reduced the features of the dataset from 64 (original features of the dataset) to 2 (newly computed features by the PCA model).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create the PCA model, with the number of principal components as argument. \n",
    "The 2 argument tells the PCA model to compute/generate 2 principal components based on all the features available in the dataset.\n",
    "'''\n",
    "pca_ncomp = PCA(n_components=2)\n",
    "\n",
    "'''\n",
    "With fit_transform(), the PCA model first learns the features of the dataset\n",
    "Then reduces (transforms) the available features into the number of principal components you specified. \n",
    "In other words, it generates the number of principal components you specified, in the format of dataframe before saving it to X_pca variable.\n",
    "'''\n",
    "X_pca_ncomp = pca_ncomp.fit_transform(X_scaled)\n",
    "\n",
    "print('The X dataframe (the dataframe consists of all features of the dataset) consists of '+ str(X.shape[1]) +' features/columns for the ' +str(X.shape[0])+' samples of the dataset.')\n",
    "print('After dimensionality reduction using PCA, the X_pca dataframe consists of '+ str(X_pca_ncomp.shape[1]) +' principal components/features created by the PCA model/columns for the ' +str(X_pca_ncomp.shape[0])+' samples of the dataset.')\n",
    "print('This means the PCA model reduced the features of the dataset from '+ str(X.shape[1]) +' (original features of the dataset) to '+ str(X_pca_ncomp.shape[1]) + ' (newly computed features by the PCA model).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the outputs from the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of principal components (features created by the PCA model) available:  2\n",
      "The percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model:\n",
      " [0.12033916 0.09561054]\n"
     ]
    }
   ],
   "source": [
    "print('The number of principal components (features created by the PCA model) available: ', pca_ncomp.n_components_)\n",
    "\n",
    "'''\n",
    "**show the percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model**\n",
    "The first value represents the percentage of variance for the 1st principal component. \n",
    "The output shows the PCA model captured the data of 1st principal component consists of 14.9% of variance.\n",
    "'''\n",
    "print('The percentage of variance (variation/useful information) of each principal component captured/calculated by the PCA model:\\n', pca_ncomp.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Develop the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the trained model:  0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train and test sets using train_test_split\n",
    "X_train_pca_ncomp, X_test_pca_ncomp, Y_train_pca_ncomp, Y_test_pca_ncomp = train_test_split(X_pca_ncomp, Y, test_size=0.2, random_state=30)\n",
    "\n",
    "# create the logistic regression model. max_iter=1000 is set here to prevent lbfgs failed to converge warning (in other words, this warning means the gradient descent cannot converge to find the minima. the optimal parameters)\n",
    "model_pca_ncomp = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# train the model\n",
    "model_pca_ncomp.fit(X_train_pca_ncomp,Y_train_pca_ncomp)\n",
    "\n",
    "# show the accuracy of the trained model\n",
    "print('The accuracy of the trained model: ',model_pca_ncomp.score(X_test_pca_ncomp,Y_test_pca_ncomp))\n",
    "\n",
    "# the accuracy here shows that by specifying the n_components to PCA, you will face the problem that too many useful features are discarded such that the accuracy is lower thant the one trained with PCA specified with percentage of variance.\n",
    "# However, the benefit of specifying the n_components to PCA is you improves the speed. (Tradeoff: Accureacy decreases, speed increases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
